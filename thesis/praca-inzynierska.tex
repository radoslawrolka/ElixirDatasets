%%%%%% -*- Coding: utf-8-unix; Mode: latex

\documentclass[polish]{aghengthesis}
%\documentclass[english]{aghengthesis} %dla pracy w języku angielskim. Uwaga, w przypadku strony tytułowej zmiana języka dotyczy tylko kolejności wersji językowych tytułu pracy. 

\usepackage{tgtermes}
\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{ragged2e}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{grffile}
\usepackage{indentfirst}
\usepackage{caption}
\usepackage{listings}
\usepackage[ruled,linesnumbered,lined]{algorithm2e}
\usepackage[bookmarks=false]{hyperref}
\usepackage{float}

\hypersetup{colorlinks,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue}

\usepackage[svgnames]{xcolor}
\usepackage{inconsolata}

\usepackage{csquotes}
\DeclareQuoteStyle[quotes]{polish}
  {\quotedblbase}
  {\textquotedblright}
  [0.05em]
  {\quotesinglbase}
  {\fixligatures\textquoteright}
\DeclareQuoteAlias[quotes]{polish}{polish}

\usepackage[nottoc]{tocbibind}

\usepackage[
style=numeric,
sorting=nyt,
isbn=false,
doi=true,
url=true,
backref=false,
backrefstyle=none,
maxnames=10,
giveninits=true,
abbreviate=true,
defernumbers=false,
backend=biber]{biblatex}
\addbibresource{bibliografia.bib}

\lstset{
    %language=Python, %% PHP, C, Java, etc.
    basicstyle=\ttfamily\footnotesize,
    backgroundcolor=\color{gray!5},
    commentstyle=\it\color{Green},
    keywordstyle=\color{Red},
    stringstyle=\color{Blue},
    numberstyle=\tiny\color{Black},    
    % morekeywords={TestKeyword},
    % mathescape=true,
    escapeinside=`',
    frame=single, %shadowbox, 
    tabsize=2,
    rulecolor=\color{black!30},
    title=\lstname,
    breaklines=true,
    breakatwhitespace=true,
    framextopmargin=2pt,
    framexbottommargin=2pt,
    extendedchars=false,
    captionpos=b,
    abovecaptionskip=5pt,
    keepspaces=true,            
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,
    tabsize=2
  }

\SetAlgorithmName{\LangAlgorithm}{\LangAlgorithmRef}{\LangListOfAlgorithms}
\newcommand{\listofalgorithmes}{\tocfile{\listalgorithmcfname}{loa}}

\renewcommand{\lstlistingname}{\LangListing}
\renewcommand\lstlistlistingname{\LangListOfListings}

\renewcommand{\lstlistoflistings}{\begingroup
\tocfile{\lstlistlistingname}{lol}
\endgroup}

% Definicje nowych rodzajów kolumn w tabeli
\newcolumntype{Y}{>{\small\centering\arraybackslash}X}
%\newcolumntype{b}{>{\hsize=1.6\hsize}Y}
%\newcolumntype{m}{>{\hsize=.6\hsize}Y}
%\newcolumntype{s}{>{\hsize=.4\hsize}Y}

\captionsetup[figure]{skip=5pt,position=bottom}
\captionsetup[table]{skip=5pt,position=top}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author{Radosław Rolka, Weronika Wojtas}

\titlePL{Biblioteka Datasets dla Eliksira}
\titleEN{Datasets library for Elixir}

\fieldofstudy{Informatyka}

%\typeofstudies{Stacjonarne}

\supervisor{dr inż.\ Aleksander Smywiński-Pohl}

\date{\the\year}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{\ChapterTitleProjectVision}
\label{sec:cel-wizja}

Celem pracy jest stworzenie biblioteki w języku Elixir, która umożliwi łatwe pobieranie, przetwarzanie i zarządzanie zbiorami danych, które są powszechnie wykorzystane w uczeniu maszynowym. Biblioteka powinna oferować szeroki wybór gotowych zbiorów danych, a także możliwość dodawania własnych.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Opis dziedziny problemu}
\label{sec:opis-dziedziny-problemu}

Praca z modelami uczenia maszynowego jest ściśle związana z wykorzystaniem danych, które stanowią podstawę procesów trenowania oraz walidacji algorytmów. Dostęp do odpowiednio przygotowanych i zróżnicowanych zbiorów danych ma kluczowe znaczenie dla jakości i skuteczności uczenia modeli. Dane muszą być nie tylko obszerne i reprezentatywne, lecz także poddane procesom czyszczenia, normalizacji oraz formatowania, co często wiąże się z dużym nakładem czasu i zasobów.
Dodatkowym wyzwaniem jest różnorodność źródeł oraz formatów danych, która utrudnia ich bezpośrednie wykorzystanie w systemach uczenia maszynowego bez wcześniejszego przetwarzania.
W odpowiedzi na te problemy coraz większe znaczenie zyskują publiczne repozytoria danych, takie jak Hugging Face Datasets. Platforma ta udostępnia bardzo dużą liczbę otwartych zbiorów danych obejmujących m.in. dane tekstowe, obrazy, nagrania dźwiękowe oraz zbiory multimodalne. Dane te są zazwyczaj ujednolicone strukturalnie i opisane metadanymi, co umożliwia ich szybkie wykorzystanie w procesie trenowania modeli. Integracja repozytorium z popularnymi bibliotekami uczenia maszynowego znacząco upraszcza pracę z danymi i redukuje czas potrzebny na ich przygotowanie.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\textwidth]{zdjęcia/zbiory.png}
  \caption[Zrzut strony HuggingFace Datasets]{Zrzut strony HuggingFace Datasets}
  \label{fig:schemat-systemu}
\end{figure}

\FloatBarrier

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motywacja}
\label{sec:motywacja}

Podczas studiów zainteresowaliśmy się językami programowania funkcyjnego, szczególnie Elixirem. Choć na początku jego podejście może wydawać się nietypowe, szybko dostrzegliśmy, jak prosty i elegancki jest ten język. Podczas pracy z Elixirem zauważyliśmy, że brakuje w nim biblioteki do zarządzania zbiorami danych, która w innych językach jest szeroko stosowana, szczególnie w sztucznej inteligencji.

Postanowiliśmy, że to będzie temat naszej pracy inżynierskiej. Chcemy stworzyć bibliotekę w Eliksirze, inspirowaną Hugging Face Datasets~\cite{huggingfaceDatasets}, która pozwoli programistom łatwiej pracować ze zbiorami danych i ułatwi dostęp do nich.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Rola produktu}
\label{sec:rola-produktu}

Głównym celem biblioteki jest uproszczenie i automatyzacja zarządzania, przetwarzania oraz optymalizacja zbiorów danych. Umożliwienie łatwego dostępu do różnorodnych zbiorów danych pozwoli na szybsze rozpoczęcie pracy nad projektami, eliminując konieczność manualnego zbierania i konfigurowania danych. Integracja funkcji automatycznego czyszczenia, normalizacji, skalowania i augmentacji danych znacząco zredukuje czasochłonne procesy przygotowywania danych, co jest zazwyczaj barierą w szybkim prototypowaniu i testowaniu modeli uczenia maszynowego. Użytkownikami końcowymi projektowanej biblioteki są przede wszystkim analitycy, specjaliści od uczenia maszynowego oraz studenci zajmujący się analizą danych i sztuczną inteligencją, którym to narzędzie ma za zadanie zwiększyć produktywność poprzez automatyzację rutynowych zadań.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Obszary funkcjonalne}
\label{sec:obszary-funkcjonalne}

\begin{enumerate}
    \item \textbf{Pobieranie i zarządzanie zbiorami danych}
    \begin{itemize}
        \item Pobieranie gotowych zbiorów danych z różnych źródeł: Implementacja mechanizmów umożliwiających pobieranie danych z platform takich jak Hugging Face Hub~\cite{huggingfaceHub}, Kaggle~\cite{kaggle} czy innych repozytoriów.
        \item Dodawanie własnych zbiorów danych: Możliwość integracji i zarządzania własnymi zestawami danych w systemie.
    \end{itemize}
    \item \textbf{Przeglądanie zbiorów danych}
    \begin{itemize}
        \item Łatwe przeglądanie dostępnych zbiorów danych: Interfejsy umożliwiające szybki podgląd i analizę dostępnych danych.
        \item Filtrowanie zbiorów danych: Narzędzia do selekcji danych na podstawie określonych kryteriów.
    \end{itemize}
    \item \textbf{Przetwarzanie i transformacja danych}
    \begin{itemize}
        \item Czyszczenie danych: Funkcje do usuwania błędów i niekompletnych rekordów.
        \item Normalizacja danych: Metody standaryzacji wartości w zbiorach danych.
        \item Tokenizacja: Proces dzielenia tekstu na mniejsze jednostki, takie jak słowa czy zdania.
        \item Tworzenie podzbiorów danych: Możliwość dzielenia większych zbiorów na mniejsze, bardziej zarządzalne części.
    \end{itemize}
    \item \textbf{Przykłady rozwiązań}
    \begin{itemize}
        \item Przykłady użycia: Praktyczne scenariusze i case studies demonstrujące zastosowanie poszczególnych funkcji.
    \end{itemize}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Wymagania niefunkcjonalne}
\label{sec:wymagania-niefunkcjonalne}

Wymagania niefunkcjonalne odgrywają kluczową rolę w zapewnieniu, że stworzona biblioteka nie tylko spełni swoje zadania funkcjonalne, ale również będzie przyjazna dla użytkownika. Poniżej przedstawiono główne wymagania niefunkcjonalne dla projektu:
\begin{itemize}
    \item Wydajność - Biblioteka powinna efektywnie zarządzać i przetwarzać duże zbiory danych z minimalnym opóźnieniem.
    \item Kompatybilność - Interfejs powinien być kompatybilny z różnymi systemami operacyjnymi i integrować się z istniejącymi popularnymi narzędziami i bibliotekami w ekosystemie Eliksira.
    \item Dokumentacja - Kompletna i zrozumiała dokumentacja techniczna jest niezbędna, by użytkownicy mogli efektywnie wykorzystywać wszystkie funkcje biblioteki.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Przegląd dostępnych rozwiązań}
\label{sec:przeglad-dostepnych-rozwiazan}

Jednym z głównych narzędzi w tej dziedzinie rozwiązań jest biblioteka datasets od Hugging Face~\cite{huggingfaceDatasets}, która jest szeroko stosowana w społeczności uczenia maszynowego. Biblioteka datasets oferuje łatwy dostęp do szerokiej gamy zbiorów danych w różnych językach programowania. Oferuje ona również różnorodne narzędzia do przetwarzania i transformacji danych.
\par
W kontekście Eliksira, który nadal jest dynamicznie rozwijającym się językiem, nie istnieje jeszcze takie narzędzie, które w pełni odpowiadałoby potrzebom użytkowników w zakresie zarządzania i przetwarzania danych dla uczenia maszynowego, co tworzy przestrzeń na rynku dla nowego rozwiązania, które może lepiej odpowiadać na unikalne potrzeby społeczności Eliksira, zwiększając efektywność ich pracy dzięki specjalizowanym narzędziom dostosowanym do ich środowiska i metod pracy.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analiza technologiczna}
\label{sec:analiza-technologiczna}

Stos technologiczny został zaprojektowany z myślą o maksymalnym wykorzystaniu możliwości języka Elixir oraz jego ekosystemu. Do obliczeń numerycznych oraz operacji na tensorach wykorzystamy bibliotekę NX~\cite{nx}. Biblioteka zapewnia wydajność w przeprowadzaniu operacji matematycznych, szczególnie w kontekście obliczeń związanych z dużymi zbiorami danych i sztuczną inteligencją. NX oferuje wsparcie dla operacji na tensorach, które są kluczowe w procesach uczenia maszynowego oraz analizy danych.

Zintegrowany z NX jest także Explorer~\cite{explorer}, który będziemy wykorzystywać w naszej pracy do efektywnego zarządzania i analizy danych. Explorer to biblioteka, która umożliwia pracę z dwoma głównymi typami struktur danych: seriami, oraz dataframe’ami. Te struktury pozwalają na wygodne i szybkie eksplorowanie danych, co jest szczególnie istotne podczas analizy informacji. Explorer, jako backend, korzysta z Polars, biblioteki napisanej w języku Rust co przekłada się na znaczną poprawę wydajności w obliczeniach z dużymi zbiorami.

Do współpracy z modelami głębokiego uczenia maszynowego w naszym projekcie zastosujemy bibliotekę Bumblebee~\cite{bumblebee}, która pozwala na łatwą integrację z pretrenowanymi modelami sieci neuronowych. Bumblebee umożliwia dostęp do popularnych modeli, które zostały udostępnione przez platformy sztucznej inteligencji, takie jak Hugging Face Transformers~\cite{huggingfaceTransformers}. Ta biblioteka umożliwi łatwą implementację i wykorzystanie zaawansowanych modeli AI w naszej pracy, co pozwoli na efektywne wdrożenie algorytmów uczenia maszynowego i głębokiego uczenia w środowisku Eliksira.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analiza ryzyka}
\label{sec:analiza-ryzyka}

W procesie projektowania i rozwijania nowej biblioteki istnieje wiele potencjalnych ryzyk, których zidentyfikowanie pozwala na lepsze przygotowanie, co z kolei zwiększa szanse na pomyślne zakończenie projektu. Są to między innymi:
\begin{itemize}
    \item Adaptacja przez społeczność - Jako że Elixir jest stosunkowo mniej popularny niż inne języki wykorzystywane w dziedzinie uczenia maszynowego, takie jak Python, istnieje ryzyko, że biblioteka nie zyska szerokiego grona użytkowników. Promocja biblioteki i demonstrowanie jej wartości w rzeczywistych projektach będzie kluczowe.
    \item Integracja z istniejącymi narzędziami - Problemy z integracją nowej biblioteki z już istniejącymi ekosystemami i narzędziami, których niekompatybilność może powstrzymać potencjalnych użytkowników przed korzystaniem z biblioteki.
    \item Obsługą dużych zbiorów danych - Możliwe, że biblioteka nie będzie w stanie efektywnie procesować dużych zbiorów danych lub że wystąpią problemy z wydajnością.
    \item Niedostateczne testowanie - Niewystarczające testowanie w różnych środowiskach i scenariuszach użytkowania może prowadzić do niezauważonych błędów, które ujawnią się dopiero po wdrożeniu biblioteki.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Podsumowanie}
\label{sec:podsumowanie}


Projekt ma na celu stworzenie biblioteki w języku Elixir, która będzie odpowiadała funkcjonalności biblioteki Hugging Face Datasets~\cite{huggingfaceDatasets}, umożliwiając łatwe pobieranie, przetwarzanie i zarządzanie zbiorami danych używanymi w uczeniu maszynowym. Biblioteka ta oferować będzie funkcje takie jak pobieranie gotowych zbiorów danych z różnych źródeł, możliwość dodawania własnych zbiorów danych, filtrowanie i przeglądanie dostępnych zbiorów, a także przetwarzanie danych (czyszczenie, normalizacja, tokenizacja). Użytkownicy będą mogli tworzyć podzbiory danych oraz integrować bibliotekę z innymi narzędziami w Eliksirze, takimi jak Nx~\cite{nx}, Explorer~\cite{explorer} i Bumblebee~\cite{bumblebee}. Projekt zakłada również dostarczenie pełnej dokumentacji oraz przykładów użycia. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{\ChapterTitleScope}
\label{sec:zakres-funkcjonalnosci}

Celem niniejszego rozdziału jest przedstawienie specyfikacji funkcjonalnej projektowanej biblioteki. Specyfikacja została opracowana na podstawie analizy potrzeb użytkowników i rozmów konsultacyjnych z zainteresowanymi stronami.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Charakterystyka użytkownika}

System zakłada istnienie jednego głównego rodzaju użytkownika – \textbf{programisty pracującego z językiem Elixir}, który zajmuje się tworzeniem, trenowaniem lub walidacją modeli uczenia maszynowego. Użytkownicy ci mogą być częścią większych zespołów badawczo-rozwojowych lub niezależnymi deweloperami.

Zakłada się, że użytkownik:
\begin{itemize}
    \item posiada podstawową lub zaawansowaną znajomość języka Elixir,
    \item zna podstawy uczenia maszynowego oraz pracy z danymi,
    \item wymaga efektywnego i prostego dostępu do przygotowanych zbiorów danych,
    \item oczekuje narzędzi ułatwiających wstępne przetwarzanie danych, takich jak czyszczenie, normalizacja, tokenizacja.
\end{itemize}
\label{sec:charakterystyka-uzytkownika}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Charakterystyka systemów współpracujących}
\label{sec:charakterystyka-systemow-wspolpracujacych}

Tworzona biblioteka do zarządzania zbiorami danych w języku Elixir zakłada ścisłą współpracę z wybranym zestawem zewnętrznych narzędzi i bibliotek, które pełnią kluczową rolę w zapewnieniu pełnej funkcjonalności systemu. Integracja tych komponentów pozwala na maksymalne wykorzystanie potencjału języka Elixir w kontekście przetwarzania danych na potrzeby uczenia maszynowego. Poniżej przedstawiono charakterystykę najważniejszych współpracujących systemów:

\begin{itemize}
    \item \textbf{Źródła zbiorów danych} – Biblioteka umożliwia pobieranie popularnych zbiorów danych wykorzystywanych w uczeniu maszynowym z różnych publicznych repozytoriów, takich jak Hugging Face Datasets~\cite{huggingfaceDatasets}. W związku z tym wymagana jest integracja z usługami HTTP i obsługą różnorodnych formatów danych.

    \item \textbf{Nx}~\cite{nx} – Biblioteka opiera się na integracji z narzędziem Nx, które zapewnia funkcje numeryczne i przetwarzanie tensorów. Współpraca z Nx pozwala na bezproblemowe przygotowanie danych do trenowania modeli uczenia maszynowego w Eliksirze.

    \item \textbf{Explorer}~\cite{explorer} – Do eksploracji, filtrowania i transformacji danych tabularycznych wykorzystywana jest biblioteka Explorer , która umożliwia przetwarzanie danych w sposób zbliżony do narzędzi takich jak Pandas~\cite{reback2020pandas} w Pythonie. Dzięki temu użytkownik może łatwo analizować i przygotowywać dane w ramach jednego ekosystemu.

    \item \textbf{Bumblebee}~\cite{bumblebee} – W celu dalszego wykorzystania przetworzonych danych, możliwa jest integracja z biblioteką Bumblebee, która dostarcza gotowe modele i narzędzia do pracy z NLP i uczeniem głębokim.

    \item \textbf{Lokalna pamięć masowa} – System wspiera lokalne przechowywanie danych oraz cache’owanie zbiorów, aby zminimalizować potrzebę wielokrotnego pobierania tych samych danych i zwiększyć wydajność pracy z dużymi zestawami.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Wymagania funkcjonalne}
\label{sec:wymagania-funkcjonalne}

W poniższym rozdziale szczegółowo opisano wymagania funkcjonalne dla projektowanej biblioteki w języku Elixir, klasyfikując je zgodnie z metodologią MoSCoW~\cite{MoSCow} ukierunkowaną na najbardziej krytyczne aspekty funkcjonalności systemu.
\newline\newline
{\noindent\large{\textbf{Must Have}}}

\begin{itemize}
    \item Możliwość pobierania datasetów z zewnętrznych źródeł – biblioteka musi umożliwić użytkownikowi łatwy dostęp do zasobów danych oferowanych przez różne repozytoria.
    \item Integracja z narzędziami Elixir takimi jak Nx i Explorer – niezbędne jest zapewnienie kompatybilności i efektywnej współpracy narzędziowej.
    \item Pełna dokumentacja funkcji biblioteki – użytkownicy muszą mieć dostęp do jasnych i zrozumiałych instrukcji korzystania z biblioteki.
\end{itemize}

{\noindent\large{\textbf{Should Have}}}
\begin{itemize}
    \item Możliwość dodawania własnych datasetów do biblioteki – funkcja ta pozwala użytkownikom na personalizację i rozszerzenie bazy danych.
    \item Narzędzia do czyszczenia i normalizacji danych – choć nie krytyczne, znacząco podnoszą wartość użytkową biblioteki.
\end{itemize}

{\noindent\large{\textbf{Could Have}}}
\begin{itemize}
    \item Rozbudowane funkcje tokenizacji danych – ułatwiłyby przetwarzanie tekstu, zwiększając potencjalne obszary zastosowań biblioteki.
    \item Wtyczki wspierające nowsze frameworki i biblioteki w ekosystemie Elixir – mogą zwiększyć atrakcyjność biblioteki dla szerokiej grupy użytkowników.
\end{itemize}

{\noindent\large{\textbf{Won't Have}}}
\begin{itemize}
    \item Automatyczne tłumaczenia dokumentacji na różne języki – choć przydatne w przyszłości, nie będą dostępne w pierwszej wersji produktu.
    \item Zaawansowane algorytmy sztucznej inteligencji do analizy danych – nie są planowane w aktualnym zakresie projektu.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Wymagania jakościowe}
\label{sec:wymagania-jakosciowe}

Poniżej przedstawione zostały wymagania niefunkcjonalne, które mają na celu zapewnienie wysokiej jakości tworzonej biblioteki oraz komfortu jej użytkowania. Odpowiednie spełnienie tych wymagań wpłynie pozytywnie na łatwość integracji, rozwój i utrzymanie projektu w dłuższym okresie.

\begin{itemize}
    \item \textbf{Czytelny i spójny interfejs API} – Interfejs biblioteki powinien być intuicyjny i dobrze zaprojektowany, umożliwiając użytkownikowi szybkie rozpoczęcie pracy bez konieczności zapoznawania się z nadmiernie rozbudowaną dokumentacją. Nazewnictwo funkcji, struktur i modułów powinno być spójne i zgodne z konwencjami języka Elixir.

    \item \textbf{Wydajność} – Biblioteka powinna umożliwiać efektywne operacje na dużych zbiorach danych, takich jak filtrowanie, czyszczenie czy transformacja. Szczególny nacisk powinien zostać położony na optymalizację operacji na dużych zbiorach dancyh oraz minimalizację zużycia pamięci i czasu przetwarzania.

    \item \textbf{Skalowalność} – Projekt powinien być skalowalny zarówno pod względem wielkości obsługiwanych danych. Powinien umożliwiać bezproblemowe dodawanie nowych źródeł danych, funkcjonalności i formatów danych bez konieczności istotnej przebudowy istniejącej architektury.

    \item \textbf{Bezpieczeństwo danych} – W przypadku integracji z zewnętrznymi źródłami danych, komunikacja powinna być realizowana z użyciem bezpiecznych protokołów (np. HTTPS). System powinien być odporny na wstrzykiwanie niepoprawnych danych oraz błędne formaty plików.

    \item \textbf{Odporność na błędy} – Biblioteka powinna zawierać mechanizmy wykrywania i obsługi błędów, umożliwiające użytkownikowi uzyskanie jasnych komunikatów w przypadku problemów z danymi, połączeniem lub działaniem funkcji.

    \item \textbf{Kompatybilność z ekosystemem Eliksira} – Projekt musi zapewniać pełną kompatybilność z innymi bibliotekami wykorzystywanymi w uczeniu maszynowym w języku Elixir, w szczególności Nx~\cite{nx}, Explorer~\cite{explorer} i Bumblebee~\cite{bumblebee}. Powinien też działać na różnych platformach systemowych wspierających środowisko Eliksira.

    \item \textbf{Dokumentacja} – Biblioteka powinna być opatrzona pełną dokumentacją techniczną, zawierającą opisy funkcji, struktur danych, przykłady użycia oraz wskazówki dotyczące integracji z innymi narzędziami. Dokumentacja powinna być dostępna zarówno w kodzie (np. jako modułowe \texttt{@doc}), jak i w formie zewnętrznej (np. README, przewodniki).

    \item \textbf{Łatwość w utrzymaniu i rozwoju} – Kod źródłowy powinien być przejrzysty, modularny, zgodny z dobrymi praktykami programistycznymi i łatwy do testowania oraz rozszerzania. Projekt powinien uwzględniać przyszłą rozbudowę, np. o nowe formaty danych, dodatkowe transformacje lub integracje.

    \item \textbf{Testowalność} – System powinien być w pełni testowalny. Moduły powinny być projektowane w sposób umożliwiający tworzenie testów jednostkowych oraz testów integracyjnych, co ułatwi utrzymanie wysokiej jakości kodu i szybką detekcję błędów w przyszłości.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scenariusze użytkowania}
\label{sec:scenariusze-uzytkowania}

Poniżej przedstawiono przykładowe scenariusze użytkowania systemu, które ilustrują typowe sytuacje, w jakich programista może korzystać z biblioteki. Scenariusze te odzwierciedlają główne funkcjonalności systemu i obrazują sposób interakcji użytkownika z interfejsem programistycznym (API) biblioteki.

\begin{itemize}
    \item \textbf{Pobranie gotowego zbioru danych}

    Użytkownik chce szybko pobrać popularny zbiór danych w celu przetestowania modelu klasyfikacji tekstu. W tym celu korzysta z funkcji udostępnianych przez bibliotekę, które automatycznie pobierają dane z repozytorium, zapisują je lokalnie i przygotowują do dalszego przetwarzania.

    \item \textbf{Dodanie własnego zbioru danych}

    Użytkownik posiada własny zbiór danych zapisany w formacie CSV. Chce go załadować, wstępnie przefiltrować oraz znormalizować. Korzystając z biblioteki, użytkownik definiuje strukturę zbioru danych, wskazuje lokalizację pliku, a następnie stosuje dostępne funkcje do oczyszczenia danych i konwersji ich do odpowiedniego formatu.

    \item \textbf{Filtrowanie danych na podstawie kryteriów}

    Użytkownik analizuje zbiór danych zawierający opinie klientów i chce utworzyć podzbiór, który zawiera wyłącznie opinie pozytywne. Biblioteka umożliwia zastosowanie funkcji filtrowania na podstawie warunków logicznych, co pozwala szybko uzyskać interesujący użytkownika fragment danych.

    \item \textbf{Integracja danych z modelem uczenia maszynowego}
    
    Po przygotowaniu danych, użytkownik chce przesłać je jako tensory do modelu zaimplementowanego w bibliotece Bumblebee~\cite{bumblebee}. Biblioteka wspiera konwersję danych do struktury kompatybilnej z Nx\cite{nx} oraz umożliwia bezpośrednie przekazanie ich do dalszego przetwarzania przez model.

    \item \textbf{Przegląd dostępnych zbiorów danych}
    
    Użytkownik nie jest jeszcze zdecydowany, z jakim zbiorem chce pracować. Korzysta z funkcji przeglądania dostępnych zbiorów, które zawierają metadane, takie jak: źródło, liczba rekordów, typ danych (tekst, obraz, liczby), wymagania wstępne itp. Po zapoznaniu się z informacjami, wybiera odpowiedni zbiór i rozpoczyna pracę.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Podsumowanie}
\label{sec:podsumowanie}

 Ten rozdział dostarcza wszechstronnej analizy projektowanego systemu, obejmującej szczegółowy opis funkcji z uwzględnieniem priorytetów, opis procesów biznesowych oraz scenariuszy użytkowania, co pozwala lepiej zrozumieć i wizualizować funkcjonalności projektu. Ta całościowa prezentacja ułatwia zarządzanie oczekiwaniami i planowanie dalszych etapów rozwoju systemu, uwzględniając ustalony zakres prac i zasoby, co jest kluczowe dla skutecznego planowania przyszłych etapów wdrożenia.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{\ChapterTitleRealizationAspects}
\label{sec:wybrane-aspekty-realizacji}

Niniejszy rozdział poświęcony jest praktycznym aspektom implementacji biblioteki. Celem tego rozdziału jest przedstawienie kluczowych decyzji projektowych, które miały wpływ na strukturę i funkcjonalności finalnego produktu. Rozdział ten stanowi podstawę do głębszego zrozumienia technicznego podejścia przyjętego podczas tworzenia biblioteki oraz wyjaśnia, jakie specyficzne problemy zostały rozwiązane w trakcie pracy nad projektem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Architektura systemu}
\label{sec:architektura-systemu}

System opracowany w ramach tej pracy inżynierskiej został skonstruowany tak, aby zapewnić użytkownikowi łatwy dostęp do popularnych zestawów danych wykorzystywanych w uczeniu maszynowym oraz umożliwić dodawanie i zarządzanie własnymi zbiorami danych. Aby osiągnąć te cele, architektura systemu została podzielona na trzy główne moduły. 
\newline\par
Moduł sieciowy jest odpowiedzialny za wszystkie operacje wymagające komunikacji z zewnętrznymi serwerami za pomocą protokołu HTTP. Jego głównymi zadaniami są: pobieranie danych z zewnętrznych źródeł, przesyłanie żądań o dostęp do zbiorów danych i odbieranie odpowiedzi. 
\newline\par
Moduł HuggingFace odpowiada za integrację z API Hugging Face, co pozwala na wyszukiwanie, pobieranie i zarządzanie zbiorami danych dostępnymi na platformie oraz autoryzację do materiałów z ograniczonym dostępem. 
\newline\par
Interfejs biblioteki ElixirDatasets to komponent, z którym bezpośrednio wchodzi w interakcję użytkownik końcowy. Stanowi on "fasadę" dla wszystkich operacji dostępnych w bibliotece, ukrywając za sobą złożoność modułów niższego poziomu. Każdy z tych komponentów pełni specyficzne funkcje, które razem tworzą spójny i efektywny system.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\textwidth]{zdjęcia/schemat-systemu.png}
  \caption[Schemat systemu]{Schemat systemu}
  \label{fig:schemat-systemu}
\end{figure}

\FloatBarrier

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Stos technologiczny}
\label{sec:stos-technologiczny}

Niniejszy rozdział opisuje technologie wybrane do budowy i wsparcia rozwoju naszej biblioteki w Eliksirze. Przedstawione zostaną zarówno główne składniki stosu technologicznego, jak i przyczyny ich wyboru, co umożliwi zrozumienie, dlaczego stanowią one optymalne rozwiązanie.

\subsection{Elixir i biblioteka standardowa}
Język programowania Elixir został wybrany jako główny język programowania ze względu na jego skalowalność, wydajność oraz wyjątkowe wsparcie dla programowania współbieżnego. Biblioteka standardowa Eliksira dostarcza szeroką gamę modułów i funkcji, które pomagają w efektywnym budowaniu aplikacji, w tym w obsłudze sieciowej, przetwarzaniu danych i konstrukcjach współbieżnych.

\subsection{Explorer}
Współpraca z danymi wymaga używania narzędzi, które umożliwiają łatwą manipulację i transformację zbiorów danych. Biblioteka Explorer służy do łatwego zarządzania danymi w Eliksirze, oferując funkcje podobne do tych z pakietu pandas w Pythonie. Umożliwia ona efektywne przetwarzanie dużych zbiorów danych.

\subsection{Mix}
Mix jest narzędziem służącym do zarządzania projektami w Eliksirze, które umożliwia kompilację kodu, jego testowanie oraz zarządzanie zależnościami. Mix jest integralną częścią ekosystemu Elixir i stanowi fundament pod względem konstrukcji i administracji projektami.

\subsection{ExDoc}
Dokumentacja jest niezmiernie ważna dla utrzymania i rozwijania projektów programistycznych, zwłaszcza tych otwartych, gdzie inni programiści mogą wnosić wkład. ExDoc to narzędzie do generowania dokumentacji w Eliksirze, które pozwala na tworzenie przejrzystej i łatwo przeszukiwalnej dokumentacji dla projektów.

\begin{figure}[!htbp]
  \centering
\includegraphics[width=.9\textwidth]{zdjęcia/exdoc.png}
\caption[Dokumentacja]{Dokumentacja}
\label{fig:progressbar}
\end{figure}

\begin{figure}[!htbp]
  \centering
\includegraphics[width=.9\textwidth]{zdjęcia/dok2.png}
\caption[Dokumentacja]{Dokumentacja}
\label{fig:progressbar}
\end{figure}

\begin{figure}[!htbp]
  \centering
\includegraphics[width=.9\textwidth]{zdjęcia/dok3.png}
\caption[Dokumentacja]{Dokumentacja w edytorze kodu}
\label{fig:progressbar}
\end{figure}

\FloatBarrier

\subsection{ExUnit i ExCoveralls}
Zapewnienie jakości kodu poprzez testy jest fundamentem stabilnego oprogramowania. ExUnit to framework testowy dostarczany razem z Elixirem, który umożliwia pisanie czytelnych i efektywnych testów jednostkowych. ExCoveralls z kolei to narzędzie, które pozwala na mierzenie pokrycia kodu testami, co jest ważnym wskaźnikiem jakości projektu programistycznego.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Przegląd poszczególnych komponentów}
\label{sec:przeglad-poszczegolnych-komponentow}

W tej sekcji dokonujemy szczegółowego przeglądu kluczowych komponentów systemu, które mają zasadnicze znaczenie dla działania i efektywności naszej biblioteki. Każdy z komponentów odpowiada za specyficzne funkcje i jest istotny zarówno z punktu widzenia realizacji podstawowych zadań, jak i możliwości rozbudowy czy utrzymania całego systemu.

\subsection{Moduł Interfejsu}
\label{sec:modul-interfejsu}

Moduł interfejsu odgrywa kluczową rolę w naszej bibliotece, zapewniając interaktywny dostęp do funkcjonalności platformy HuggingFace/datasets. Ten komponent ma za zadanie odtworzenie interfejsu tej platformy, co umożliwia użytkownikom łatwe pobieranie zbiorów danych, ich udostępnianie oraz przeglądanie szczegółowych informacji o danych przed ich pobraniem.
\newline \par
Interaktywność modułu interfejsu znacząco ułatwia korzystanie z bogatych zasobów dostępnych na platformie Hugging Face, pozwalając na bezpośrednie i intuicyjne wykorzystanie dostępnych zbiorów w procesach analitycznych i badawczych. Dzięki temu użytkownicy mogą nie tylko efektywnie zarządzać danymi, ale również optymalizować czas potrzebny na przygotowanie i przetwarzanie informacji niezbędnych do analiz lub treningów modeli uczenia maszynowego.
\newline \par
Podsumowując, moduł interfejsu jest niezbędnym elementem biblioteki, umożliwiającym integrację z zewnętrznymi źródłami danych i zapewniającym użytkownikowi dostęp do funkcjonalności kluczowych dla efektywnego wykorzystania dostępnych zbiorów danych.

\subsection{Moduł HuggingFace}
\label{sec:modul-huggingface}

Moduł HuggingFace w bibliotece ElixirDatasets pełni kluczową rolę w zapewnianiu dostępu do szerokiej gamy zbiorów danych dostępnych na platformie Hugging Face. Ten komponent modułu jest odpowiedzialny za interakcję z API Hugging Face, co ułatwia pobieranie danych, ich udostępnianie, a także podgląd informacji o zbiorach przed ich pobraniem.
\newline \par
Dzięki wykorzystaniu funkcji `file\_url` i `file\_listing\_url`, moduł umożliwia uzyskanie URL-i do konkretnych plików i listowania zawartości repozytorium na platformie Hugging Face. Obejmuje to zarówno publiczne zbiory danych, jak i te prywatne, dostępne tylko dla autoryzowanych użytkowników. Proces ten jest wspomagany przez mechanizm cache'owania, gdzie każde pobrane pliki są zapisywane lokalnie wraz z ich metadanymi, co pozwala na optymalizację kolejnych zapytań.
\newline \par
Dodatkowo, rozszerzona funkcja `cached\_download` pozwala na pobieranie plików z zastosowaniem cache bazującego na ETagach (Entity Tags). Jeśli zasób nie uległ zmianie, dane mogą być serwowane bezpośrednio z lokalnego cache, co znacznie przyspiesza dostęp i redukuje zużycie zasobów sieciowych.
\newline \par
Użytkownik może również korzystać z opcjonalnych ustawień, takich jak tryb offline, który zapewnia dostęp do zasobów nawet w przypadku braku połączenia z Internetem, pod warunkiem że dane zostały wcześniej pobrane i zapisane w cache'u.

\subsection{Moduł sieciowy}
\label{sec:modul-sieciowy}

Moduł sieciowy w projekcie ElixirDatasets pełni fundamentalną rolę w umożliwianiu komunikacji sieciowej, stosując się do wiodących standardów praktyk internetowych. Jest odpowiedzialny za mechaniczne aspekty pobierania danych, obsługę protokołów HTTP/HTTPS oraz zarządzanie połączeniami sieciowymi. Moduł ten, zaimplementowany w `ElixirDatasets.Utils.HTTP`, służy jako centralny punkt dla wszystkich operacji sieciowych wykonanych przez bibliotekę.
\newline
Fundamentalne funkcje modułu networkingowego obejmują:
\begin{itemize}
    \item \textbf{Pobieranie danych:} Moduł potrafi wykonywać bezpieczne żądania do zdalnych serwerów, pobierając dane bezpośrednio na lokalne dyski użytkownika. To mechanizm, który stoi za funkcją `download`, pozwala na efektywne zarządzanie przepływem danych.
    \item \textbf{Zarządzanie nagłówkami HTTP:} Za pomocą funkcji `get\_header`, moduł umożliwia manipulację i odczyt nagłówków HTTP, co jest kluczowe do prawidłowego rozumienia i kontrolowania odpowiedzi serwera.
    \item \textbf{Opcje konfiguracyjne:} Użytkownicy mogą dostosować wiele aspektów żądań HTTP, takich jak nagłówki, ciało żądania, czy timeout, co daje elastyczność potrzebną do obsługi różnorodnych scenariuszy użytecznych.
\end{itemize}

Jedną z istotnych cech modułu jest jego zdolność do obsługi przekierowań i autoryzacji, co czyni go potężnym narzędziem do integracji z różnymi API oferującymi dane. Ponadto, moduł zapewnia mechanizmy bezpieczeństwa, takie jak weryfikacja certyfikatów SSL czy obsługa etykiet ETag, które minimalizują ryzyko przechwycenia danych i umożliwiają inteligentne zarządzanie cache'owaniem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ciekawsze algorytmy i mechanizmy systemu}
\label{sec:ciekawsze-algorytmy-i-mechanizmy-systemu}

Poniższa sekcja koncentruje się na przedstawieniu najbardziej istotnych komponentów opracowywanego systemu. Zostaną one omówione nie tylko pod kątem szczegółowego działania, ale także w kontekście ich wpływu na całość tworzonego systemu.

\subsection{Cacheowanie danych}
\label{sec:cacheowanie-danych}

Cacheowanie danych jest krytycznym aspektem naszego systemu, zwiększającym jego efektywność poprzez redukcję liczby koniecznych operacji wejścia/wyjścia, zewnętrznych zapytań API oraz pobrań danych, które już wcześniej zostały załadowane. Aby zminimalizować niepotrzebne operacje, szczególnie w przypadku wielokrotnych uruchomień kodu przez użytkownika, implementujemy mechanizm etag (entity tag) oraz OId (Object Identifier). Etag i OId to unikalne identyfikatory przypisane do każdej wersji zasobu, które umożliwiają jednoznaczne stwierdzenie, czy przechowywana wersja zasobu nadal jest aktualna.
\newline \par
Przykładem wykorzystania etagu oraz oidu może być kontrola stanu zbiorów danych na zdalnym repozytorium. Poniżej przedstawiono przykłady odpowiedzi serwera, ilustrującej zastosowanie etagu i oidu:

\begin{lstlisting}[language=Python,float=!htbp,caption={[Podgląd odpowiedzi serwera z etagiem]Podgląd odpowiedzi serwera z etagiem},label=lst:odpowiedz-ze-stanem-etag]
{
  "etag": "W/\"129-cBr/GjmAu235zSmcAE8hRzjbA90\"",
  "url": "https://huggingface.co/api/datasets/fka/awesome-chatgpt-prompts"
}%                 
\end{lstlisting}

\begin{lstlisting}[language=Python,float=!htbp,caption={[Podgląd odpowiedzi serwera z oid]Podgląd odpowiedzi serwera z oid},label=lst:odpowiedz-ze-stanem-oid]
[
    {
        "type": "file",
        "oid": "f4f3945bd7150d3e12988485c42da1f8c29c59f8",
        "size": 2265,
        "path": ".gitattributes"
    },
    {
        "type": "file",
        "oid": "d3dc1fd0061ff5ebb4c34451bd6c17d4547b6612",
        "size": 339,
        "path": "README.md"
    },{
        "type": "file",
        "oid": "6df098dd5d2ff6d9fedd3aa052e6fd49c3389b77",
        "size": 104186,
        "path": "prompts.csv"
    }
]%   
\end{lstlisting}

\subsection{Pasek Postępu (ProgressBar)}
\label{sec:progress-bar}

Aby ułatwić użytkownikom wizualizację postępu pobierania dużych zbiorów danych, w bibliotece został zaimplementowany pasek postępu (ProgressBar). Jest to narzędzie graficzne, które dynamicznie aktualizuje się w trakcie pobierania danych, pokazując użytkownikowi w sposób bezpośredni ile danych już zostało pobrane, a ile pozostało do końca procesu. Taka wizualizacja jest szczególnie przydatna w przypadku pobierania dużych zbiorów danych, gdzie proces może trwać znaczną ilość czasu.

\begin{figure}[!htbp]
  \centering
\includegraphics[width=.9\textwidth]{zdjęcia/ProgressBar.png}
\caption[Zas]{Przykład wizualizacji paska postępu}
\label{fig:progressbar}
\end{figure}

\FloatBarrier

\subsection{Przykłady LiveBook}
\label{sec:przyklady-livebook}

LiveBook to narzędzie interaktywne, zaprojektowane do tworzenia dokumentów, które mogą zawierać zarówno treść edukacyjną jak i wykonywalny kod. W ramach naszej biblioteki w języku Elixir, zintegrowaliśmy przykłady LiveBook, które umożliwiają użytkownikom eksperymentowanie z kodem na żywo, bezpośrednio w przeglądarce.

Te interaktywne dokumenty są niezwykle przydatne w edukacji i prezentacji możliwości biblioteki, ponieważ pozwalają na natychmiastowe obserwowanie wyników działania kodu. Użytkownicy mogą modyfikować lub rozwijać przykłady, co zwiększa ich zrozumienie działania biblioteki i zachęca do głębszego eksplorowania jej funkcji.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\textwidth]{zdjęcia/livebook.png}
  \caption{Przykład wykorzystania LiveBook}
  \label{fig:livebook}
\end{figure}

\FloatBarrier

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Zapewnienie jakości}
\label{sec:zapewnienie-jakosci}

W projektowaniu oprogramowania, zapewnienie jakości odgrywa kluczową rolę w sprawdzaniu, czy produkt spełnia oczekiwania użytkowników i wewnętrzne standardy jakości przed jego wydaniem. W kontekście rozwijania biblioteki przedstawione zostaną techniki testowania automatycznego oraz utrzymywania jakości oprogramowania.

\subsection{Testy jednostkowe}
\label{sec:testy-jednostkowe}

Testy jednostkowe koncentrują się na pojedynczych, izolowanych fragmentach kodu, takich jak funkcje lub metody, sprawdzając, czy zachowują się one zgodnie z oczekiwaniami na podstawie zdefiniowanych przypadków testowych. W projektowanej bibliotece w języku Elixir, testy te są realizowane z wykorzystaniem frameworka ExUnit, który oferuje wsparcie dla asercji, testowania równoległego, a także umożliwia łatwą integrację z narzędziami do pomiaru pokrycia kodu testami, takimi jak ExCoveralls, którego pokrycie testami zostało ustawione na 60\%. Efektywne wykorzystanie testów jednostkowych pozwala na szybką identyfikację i izolację błędów oraz zapewnia stabilność kodu poprzez ciągłą weryfikację jego poprawności w miarę jego rozwoju.

\subsection{Analiza statyczna kodu}
\label{sec:analiza-statyczna-kodu}

W napisanej bibliotece zostały wykorzystane narzędzia analizy statycznej kodu, które przeprowadzało rygorystyczne sprawdzanie typów oraz wykrywanie nieosiągalnych fragmentów kodu. Z uwagi na dynamiczną naturę Eliksira, narzędzie to stanowi kluczowy element zapewniający dodatkowe bezpieczeństwo typów, które nie jest domyślnie ścisłe w tym języku. Ponadto, w procesie tworzenia biblioteki wykorzystano narzędzia zapewniające przestrzeganie ujednoliconego stylu kodowania, co znacząco przyczyniło się do poprawy czytelności kodu. Te narzędzia automatycznie egzekwowały zasady dotyczące formatowania i struktury kodu, co ułatwiło współpracę w zespole i zwiększyło efektywność w utrzymaniu oraz rozwijaniu projektu. Implementacja analizy statycznej pozwala na wcześniejsze zidentyfikowanie problemów oraz ułatwia utrzymanie wysokiej jakości kodu.

\subsection{Ciągła integracja (CI)}
\label{sec:ciagla-integracja}

Kluczowym narzędziem usprawniającym proces ciągłej integracji jest GitHub Actions. Ta platforma CI/CD pozwala na automatyczne wykonywanie różnorodnych operacji w arbitralnie skonfigurowanym środowisku wirtualnym. W naszym przypadku, każde zgłoszenie do repozytorium inicjuje serię zadań w GitHub Actions, które obejmują kompilację kodu w trybie ścisłym, uruchomienie testów, zgodność z ustanowionymi standardami formatowania, a także analizę pokrycia testami.

\subsection{Code review}
\label{sec:code-review}

Code review, czyli przegląd kodu prez innego członka zespołu, jest niezastąpionym procesem w cyklu rozwoju oprogramowania. Proces ten polegał na ocenie kodu przez osobę niezwiązaną bezpośrednio z implementacją przy utworzeniu żądania dodania nowych funkcjonalności do oficjalnej gałęzi repozytorium. Dopiero po pozytywnym rozpatrzeniu takiego wniosku możliwe było połączenie dwóch wersji.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Podsumowanie}
\label{sec:podsumowanie}
Biblioteka została zaprojektowana zgodnie z zasadami modularności, co klarownie definiuje rolę i odpowiedzialności poszczególnych segmentów kodu. Narzędzia oraz technologie użyte do jej rozwoju są powszechnie akceptowane i standardowe w społeczności deweloperów Eliksira. Dla zapewnienia niezawodności systemu stosowane są różnorodne formy testowania, a także regularne recenzje kodu wewnątrz zespołu. Jakość kodu jest dodatkowo monitorowana przez narzędzia do statycznej analizy, co potwierdza wysokie standardy jego utrzymania.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{\ChapterTitleWorkOrganization}
\label{sec:organizacja-pracy}

Ten rozdział poświęcony jest szczegółowemu przedstawieniu metodyki i technik, które zostały wykorzystane w trakcie realizacji projektu. Obejmuje on dokładny opis kolejnych etapów prac, a także sposób podziału odpowiedzialności i zadań w zespole projektowym. Ponadto, wskazuje i charakteryzuje narzędzia, które wspierały komunikację i zarządzanie organizacją pracy w trakcie procesu tworzenia.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Charakterystyka projektu}
\label{sec:charakterystyka-projektu}

Tworzona praca inżynierska ma charakter rozwojowy. Jej celem było stworzenie wyspecjalizowanej biblioteki w języku Elixir, koncentrującej się na efektywnym pozyskiwaniu, zarządzaniu i ładowaniu zbiorów danych powszechnie stosowanych w dziedzinie uczenia maszynowego.

Projekt od początku zakładał stworzenie solidnego fundamentu do pracy z danymi, opierając się na architekturze języka Elixir. Wymagania dotyczące kluczowych obszarów funkcjonalnych, takich jak mechanizmy pobierania danych z zewnętrznych repozytoriów ($np.$ Hugging Face Hub) oraz metody ich efektywnego ładowania do pamięci, były znane i stanowiły trzon planu prac. Projekt ten koncentrował się na dostarczeniu narzędzia umożliwiającego szybki start pracy analitycznej, automatyzując etap pozyskiwania i przygotowania danych źródłowych.

Realizacja projektu przebiegała zgodnie z procesem przyrostowo-iteracyjnym. Prace rozpoczęto od fazy badawczo-prototypowej, której celem było potwierdzenie wykonalności kluczowych założeń architektonicznych i technicznych w środowisku Elixir. Następnie, implementacja właściwa została podzielona na logiczne przyrosty (moduły), obejmujące kolejno funkcjonalności związane z pobieraniem i zarządzaniem danymi oraz ich ładowaniem i wstępnym przeglądaniem. Każdy moduł był rozwijany i testowany w krótkich iteracjach, co pozwoliło na sukcesywne budowanie stabilnej biblioteki i minimalizację wystąpienia błędów. Takie podejście umożliwiło optymalne wykorzystanie zasobów i skupienie się na dostarczeniu stabilnego rdzenia systemu.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Osoby w projekcie}
\label{sec:osoby-w-projekcie}

\noindent Projekt był realizowany przez zespół składający się z dwóch członków:
\begin{itemize}
    \item Radosław Rolka
    \item Weronika Wojtas
\end{itemize}
Opiekę merytoryczną oraz rolę klienta pełnił dr inż. Aleksander Smywiński-Pohl.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Zespół i podział obowiązków}
\label{sec:zespol-i-podzial-obowiazkow}

Projekt realizowany był przez zespół dwuosobowy, w którym każdy z członków odpowiadał za określone obszary funkcjonalności biblioteki. Podział obowiązków był zaplanowany w taki sposób, aby maksymalnie wykorzystać umiejętności każdego członka zespołu oraz zapewnić efektywny przebieg prac.
\newline\newline
\noindent\textbf{Radosław Rolka} odpowiadał za następujące obszary projektu:
\begin{itemize}
    \item \textbf{Architektura systemu} – projektowanie ogólnej struktury biblioteki oraz definiowanie interakcji między poszczególnymi modułami.
    \item \textbf{Moduł sieciowy} – implementacja funkcji odpowiedzialnych za pobieranie danych z zewnętrznych źródeł poprzez protokół HTTP/HTTPS.
    \item \textbf{Moduł HuggingFace} – integracja z API platformy Hugging Face Hub, obsługa autoryzacji oraz zarządzanie dostępem do zbiorów danych.
    \item \textbf{System cacheowania} – implementacja mechanizmu cacheowania danych bazującego na etagach i OId, optymalizacja wydajności pobierania.
    \item \textbf{Transformacja danych} – implementacja funkcji do wczytywania i przetwarzania danych z różnych formatów (TXT, CSV, JSON, Parquet).
    \item \textbf{Główny interfejs biblioteki} – tworzenie publicznego API biblioteki.
    \item \textbf{Ciągła integracja (CI/CD)} – konfiguracja GitHub Actions, automatyzacja testów i kontrola jakości kodu.
\end{itemize}

\noindent\textbf{Weronika Wojtas} odpowiadała za następujące obszary projektu:
\begin{itemize}
    \item \textbf{Architektura systemu} – projektowanie ogólnej struktury biblioteki oraz definiowanie interakcji między poszczególnymi modułami.
    \item \textbf{Integracja z Explorer} – zapewnienie kompatybilności biblioteki z narzędziem Explorer do pracy z dataframe'ami i seriami.
    \item \textbf{Przetwarzanie strumieniowe} – dodanie możliwości ładowania zbiorów strumieniowo
    \item \textbf{Wielowątkowe pobieranie zbiorów danych} – dodanie obsługi wielu wątków przy równoległym pobieraniu zbiorów
    \item \textbf{Rozszerzone funkcjonalności ładowania zbiorów danych} – dodanie dodatkowych opcji ładowania, takich jak filtrowanie zbiorów danych na podstawie nazwy, pobieranie wybranych podzbiorów danych oraz weryfikacja integralności i kompletności pobranych zbiorów
    \item \textbf{Testy jednostkowe} – pisanie kompleksowych testów jednostkowych dla wszystkich modułów biblioteki z użyciem frameworka ExUnit.
    \item \textbf{Dokumentacja} – tworzenie dokumentacji technicznej i przykładów użycia kodu w formie LiveBook.
    \item \textbf{Pokrycie testami} – monitorowanie i utrzymanie wysokiego poziomu pokrycia kodu testami (target: 60\%) z wykorzystaniem ExCoveralls.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Organizacja prac i wykorzystane narzędzia}
\label{sec:organizacja-prac-i-wykorzystane-narzedzia}

W tej sekcji opisane zostały metody i narzędzia wykorzystane do organizacji pracy nad tworzoną biblioteką. Zawiera ona opis przyjętej metodyki oraz kanałów komunikacji, a także listę kluczowych narzędzi wspierających proces programowania, kontroli wersji oraz zarządzania zadaniami.

\subsection{Metodyka i Organizacja Pracy}

W projekcie zastosowano metodykę iteracyjną w zakresie planowania i przeglądu postępów. Kluczowe elementy organizacji pracy obejmowały:

\begin{itemize}
    \item \textbf{Zarządzanie Zadaniami}: Do tworzenia, przypisywania i śledzenia zadań wykorzystywano wbudowany w repozytorium system GitHub Issues. 
    \item \textbf{Kontrola Wersji}: Do zarządzania kodem źródłowym oraz współpracy używano systemu kontroli wersji Git oraz platformy GitHub. Dzięki temu możliwe było tworzenie rozdzielnych gałęzi roboczych (branches), przeprowadzanie Code Review oraz weryfikowanie zmian przed ich włączeniem do głównej gałęzi ($main$).
\end{itemize}

% Todo: dodaj screenshot GHIssues
% Todo: dodaj screenshot z repo strona główna

Podział i przydział zadań odbywał się na regularnych spotkaniach, najczęściej zaraz po omówieniu postępów i rozwiązaniu napotkanych problemów.

\subsection{Komunikacja i Spotkania}

Komunikacja była realizowana na dwóch płaszczyznach: wewnątrz zespołu oraz z opiekunem (klientem).

Komunikacja codzienna i omawianie pilnych spraw odbywało się głównie za pośrednictwem komunikatorów Facebook Messenger oraz Discord. Kanały te służyły do szybkiego rozwiązywania bieżących problemów technicznych i organizacyjnych. Uzupełnieniem komunikacji zdalnej były regularne spotkania stacjonarne, które odbywały się co kilka tygodni. Spotkania te były kluczowe dla synchronizacji strategicznych działań, dogłębnego omówienia architektury systemu, a także wspólnego pisania i edytowania tekstu pracy inżynierskiej.

% Todo: dodaj screenshot z messenger

Kontakt z opiekunem odbywał się w kluczowych momentach realizacji projektu. Pierwsze spotkanie miało charakter konsultacyjny, służąc ostatecznemu sprecyzowaniu celów i zakresu funkcjonalnego biblioteki na etapie definiowania wymagań. Kolejne kontakty były wykorzystywane do cyklicznych prezentacji postępu prac. Taki rytm współpracy pozwalał na wczesne uzyskanie informacji zwrotnej i weryfikację, czy kierunek rozwoju systemu jest zgodny z pierwotnymi założeniami.

\subsection{Narzędzia Wykorzystane w Projekcie}

Do stworzenia, zarządzania i dokumentowania projektu wykorzystano szereg narzędzi, które można podzielić na kilka kategorii:

\begin{table}[H]
\centering
\begin{tabular}{|p{3.5cm}|p{3.5cm}|p{7cm}|}
\hline
\textbf{Obszar} & \textbf{Narzędzia} & \textbf{Opis} \\ \hline

Kontrola wersji i zarządzanie & Git / GitHub &
Hostowanie kodu źródłowego, kontrola wersji, tworzenie pull requestów oraz
zarządzanie zadaniami poprzez GitHub Issues. \\ \hline

Środowisko programistyczne & IntelliJ IDEA / Visual Studio Code &
Główne środowiska programistyczne ($IDE$) wykorzystywane do tworzenia kodu w języku Elixir. \\ \hline

Dokumentacja i testowanie & Livebook &
Narzędzie do tworzenia interaktywnych notatników w Eliksirze, wykorzystane do
prezentacji i tworzenia przykładów zastosowania biblioteki. \\ \hline

Dokumentacja końcowa & Overleaf &
Platforma chmurowa wykorzystana do wspólnego pisania i formatowania
niniejszej pracy inżynierskiej w LaTeX-u. \\ \hline

\end{tabular}
\caption{Zestawienie wykorzystanych narzędzi projektowych}
\end{table}

Wykorzystanie natywnych funkcji platformy GitHub (Issues i Git) do zarządzania zadaniami oraz kontroli wersji okazało się wystarczające dla projektu o ograniczonym, jednoosobowym/dwuosobowym zespole, eliminując konieczność stosowania bardziej złożonych narzędzi typu Jira czy Trello.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Zastosowane techniki i praktyki}
\label{sec:zastosowane-techniki-i-praktyki}

W trakcie realizacji projektu zespół stosował szereg technik i praktyk mających na celu zapewnienie wysokiej jakości kodu, efektywności pracy oraz łatwości utrzymania projektu w przyszłości. Poniżej opisano najważniejsze z nich.

\subsection{Ciągła integracja (CI/CD) z GitHub Actions}

Projekt wykorzystuje platformę GitHub Actions do automatyzacji procesów ciągłej integracji. Każde push'owanie do głównej gałęzi repozytorium oraz każde pull request'owanie inicjuje serię testów i kontroli jakości w arbitralnie skonfigurowanym środowisku wirtualnym. Główne zadania wykonywane w ramach CI/CD to:

\begin{itemize}
    \item \textbf{Kompilacja i testy} – Kod jest kompilowany, a następnie uruchamiane są wszystkie testy jednostkowe.
    \item \textbf{Kontrola formatowania} – Automatycznie sprawdzane jest, czy kod spełnia standardy formatowania.
    \item \textbf{Sprawdzanie ostrzeżeń} – Kompilacja w trybie surowym, co oznacza, że wszelkie ostrzeżenia są traktowane jako błędy i uniemożliwiają pomyślne ukończenie pipeline'u.
\end{itemize}

% Todo : dodaj zdjęcie CI run'a

\subsection{Moduledoc - Dokumentacja i przykłady w kodzie}

Każdy moduł publiczny oraz funkcja publiczna jest opatrzona dokumentacją w kodzie oraz przykładami użycia. Podane opisy są automatycznie przetwarzane do postaci ustandaryzowanej dokumentacji w ekosystemie, a następnie jest weryfikowane ich działanie oraz zgodność z implementacją, co zapobiega powstawaniu nieaktualnych lub błędnych informacji w dokumentacji. Przykładowa dokumentacja funkcji zawiera następujące elementy:

\begin{itemize}
    \item Opis przeznaczenia i zastosowania
    \item Podpisy funkcji ze specyfikacją typów
    \item Przykłady użycia
    \item Informacje o błędach, które funkcja może zwrócić
\end{itemize}

% Todo : dodaj zdjęcie module doca
% Todo : dodaj zdjęcie wygenerowanej dokumentacji

\subsection{Refaktoryzacja kodu}
Regularna refaktoryzacja kodu była integralną częścią procesu rozwoju biblioteki. Zespół dążył do utrzymania kodu w czystej i zrozumiałej formie, co ułatwiało jego dalszy rozwój i utrzymanie. Refaktoryzacja obejmowała między innymi:
\begin{itemize}
    \item Uproszczenie złożonych funkcji i modułów
    \item Usuwanie redundantnego kodu
    \item Poprawę czytelności poprzez lepsze nazewnictwo i strukturę kodu
\end{itemize}

\subsection{Pair programming}
W niektórych kluczowych momentach projektu, zespół stosował technikę pair programming, gdzie dwóch programistów wspólnie pracowało nad jednym zadaniem. Ta praktyka pozwalała na szybsze rozwiązywanie problemów, wymianę wiedzy oraz poprawę jakości kodu poprzez bieżącą weryfikację i dyskusję nad implementowanymi rozwiązaniami. Dzięki temu zespół mógł efektywniej radzić sobie z trudnymi wyzwaniami technicznymi oraz zwiększyć spójność kodu.

\subsection{Test Driven Development (TDD)}
Zespół stosował podejście Test Driven Development (TDD), które polega na pisaniu testów jednostkowych przed implementacją funkcjonalności. Ta praktyka pozwalała na lepsze zrozumienie wymagań oraz zapewniała, że każda nowa funkcjonalność była odpowiednio przetestowana od samego początku. TDD pomagało również w utrzymaniu wysokiej jakości kodu oraz ułatwiało refaktoryzację, ponieważ istniała pewność, że zmiany nie wprowadzą regresji. Przebieg TDD obejmował następujące kroki:
\begin{itemize}
    \item \textbf{Red Phase}: Napisanie testu jednostkowego, który definiuje oczekiwaną funkcjonalność
    \item \textbf{Green Phase}: Uruchomienie testu, który początkowo powinien zakończyć się niepowodzeniem, a następnie implementacja minimalnej ilości kodu potrzebnej do przejścia testu
    \item \textbf{Refactor Phase}: Refaktoryzacja kodu w celu poprawy jego struktury i czytelności, przy jednoczesnym zachowaniu przejścia wszystkich testów
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Przebieg prac}
\label{sec:przebieg-prac}

Realizacja projektu została podzielona na trzy główne, sekwencyjne fazy, które odzwierciedlają przyjętą metodykę przyrostowo-iteracyjną oraz specyfikę tworzenia biblioteki programistycznej.

\subsection{Faza Koncepcyjna i Planowania Architektury}

Faza ta obejmowała wstępny okres projektu, koncentrując się na badaniach, analizie i planowaniu strategicznym:

\begin{itemize}
    \item Definicja Zakresu i Celu: Ostateczne sprecyzowanie celu pracy jako stworzenia biblioteki do pobierania, zarządzania i ładowania zbiorów danych dla uczenia maszynowego w języku Elixir.
    \item Wybór Stosu Technologicznego: Zatwierdzenie języka Elixir jako głównego narzędzia oraz wybór kluczowych bibliotek i technik niezbędnych do efektywnej obsługi operacji $I/O$, zarządzania pamięcią, oraz przetwarzania zbiorów danych.
    \item Opracowanie modułowej struktury biblioteki. Zdecydowano o podziale na odrębne moduły odpowiedzialne za integrację z API zewnętrznymi (pobieranie), oraz parsowanie i ładowanie danych do wewnętrznego formatu.
\end{itemize}

\subsection{Faza Implementacji}

Faza implementacyjna była realizowana iteracyjnie, skupiając się na budowie i testowaniu dwóch głównych przyrostów funkcjonalnych:

\begin{itemize}
    \item \textbf{Implementacja modułu pobierania i zarządzania} - prace rozpoczęto od budowy fundamentu biblioteki, koncentrując się na interakcji ze światem zewnętrznym. Pierwszy etap polegał na integracji ze źródłami danych. Opracowano mechanizmy komunikacji z zewnętrznymi repozytoriami. Równolegle wdrożono funkcje zarządzania plikami, tworząc mechanizmy do obsługi pobranych zbiorów danych na dysku, w tym weryfikacji integralności plików oraz podstawowego zarządzania ścieżkami dostępu.
    \item \textbf{Implementacja modułu ładowania i parsowania} - po zapewnieniu możliwości dostępu do danych, skupiono się na ich efektywnym wczytywaniu. Opracowano parsery danych, wdrażając wydajne funkcje do parsowania popularnych formatów. Fazę tę zakończyło stworzenie przykładów użycia i funkcjonalności w środowisku Livebook oraz na serwerze Eliksira, które miały posłużyć do weryfikacji i demonstracji kluczowych funkcji biblioteki.
\end{itemize}

\subsection{Faza Testowania, Optymalizacji i Dokumentacji}

Ostatnia faza koncentrowała się na stabilizacji, weryfikacji i finalizacji produktu:

\begin{itemize}
    \item \textbf{Testowanie funkcjonalne}: przeprowadzono testy jednostkowe kluczowych modułów (pobieranie, parsowanie) oraz testy integracyjne, sprawdzające spójne działanie całej biblioteki.
    \item \textbf{Optymalizacja wydajności}: skupiono się na identyfikacji i usunięciu potencjalnych wąskich gardeł w operacjach $I/O$, dążąc do jak najwydajniejszego ładowania dużych zbiorów danych.
    \item \textbf{Dokumentacja}: całość prac została podsumowana w niniejszej pracy inżynierskiej, a dodatkowo utworzono szczegółową dokumentację techniczną biblioteki, opisującą interfejsy modułów i sposób instalacji.
\end{itemize}

%todo zdjęcie harmonogramu/timelineu

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Podsumowanie}
\label{sec:podsumowanie}

Prace nad projektem były starannie zaplanowane i zorganizowane, co pozwoliło na efektywne wykorzystanie zasobów zespołu oraz osiągnięcie założonych celów w określonym czasie. Wykorzystanie narzędzi organizujących i synchronizujących pracę zespołu było kluczowe dla sukcesu projektu, umożliwiając płynną komunikację i skuteczne zarządzanie zadaniami. Przyjęte techniki i praktyki programistyczne zapewniły wysoką jakość kodu oraz stabilność finalnego produktu.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{\ChapterTitleResults}
\label{sec:wyniki-projektu}

Rozdział ten podsumowuje prace wykonane w ramach realizacji systemu oraz prezentuje osiągnięte rezultaty. Ponadto zawiera ocenę uzyskanych wyników dokonaną przez członków zespołu, a także wskazuje możliwe kierunki dalszego rozwoju opracowanego rozwiązania.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Zrealizowane funkcjonalnosci}
\label{sec:zrealizowane-funkcjonalnosci}

Zgodnie z przyjętą metodyką realizacji projektu, w pierwszej kolejności zaimplementowano funkcjonalności o najwyższym priorytecie, stanowiące podstawę działania tworzonej biblioteki. Obejmują one możliwość ładowania zbiorów danych zarówno z repozytorium Hugging Face Hub, jak i z lokalnego systemu plików, z automatycznym wykrywaniem obsługiwanych formatów danych oraz wsparciem dla różnych konfiguracji i podziałów zbiorów danych.

Istotnym elementem systemu jest mechanizm cache’owania, który umożliwia lokalne przechowywanie pobranych danych i ogranicza konieczność ich ponownego pobierania. Zaimplementowano różne tryby pracy z cache oraz możliwość konfiguracji katalogu przechowywania danych, a uzupełnieniem tej funkcjonalności jest tryb pracy offline.

Biblioteka oferuje również wsparcie dla strumieniowego przetwarzania danych, umożliwiającego efektywną pracę z dużymi zbiorami bez konieczności ładowania ich w całości do pamięci operacyjnej. W celu zwiększenia wydajności zaimplementowano mechanizmy przetwarzania równoległego, pozwalające na szybsze ładowanie zbiorów danych składających się z wielu plików.

System zapewnia integrację z interfejsem programistycznym Hugging Face Hub, umożliwiając pobieranie informacji i metadanych dotyczących zbiorów danych, a także obsługę autoryzacji, w tym dostęp do prywatnych zasobów. Rozszerzeniem funkcjonalności biblioteki jest możliwość wysyłania zbiorów danych do repozytorium Hugging Face, co pozwala na ich publikowanie i zarządzanie bezpośrednio z poziomu języka Elixir.

Dane mogą być automatycznie konwertowane do struktury DataFrame z wykorzystaniem biblioteki Explorer. Całość została uzupełniona o mechanizmy walidacji parametrów, obsługę błędów oraz zestaw testów i dokumentację API. Szczegółowe scenariusze użycia biblioteki zostały opisane w kolejnych rozdziałach pracy.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\textwidth]{zdjęcia/hex.png}
  \caption[Publikacja w Hex]{Publikacja biblioteki w hex}
  \label{fig:schemat-systemu}
\end{figure}

\FloatBarrier

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Główne scenariusze uzytkowania}
\label{sec:glowne-scenariusze-uzytkowania}

Poniższa sekcja przedstawia najważniejsze scenariusze użycia biblioteki ElixirDatasets z perspektywy programisty korzystającego z niej w środowisku języka Elixir. Opisane scenariusze ilustrują typowe sposoby pracy ze zbiorami danych pochodzącymi z repozytorium Hugging Face oraz z lokalnego systemu plików, a także prezentują mechanizmy optymalizacji wydajności, takie jak cache’owanie, przetwarzanie strumieniowe oraz równoległe ładowanie danych. Dla wybranych scenariuszy dołączono fragmenty kodu demonstrujące sposób wykorzystania interfejsu API biblioteki.

\subsection{Ładowanie zbioru danych z repozytorium Hugging Face}

Podstawowym scenariuszem użycia biblioteki jest pobranie zbioru danych bezpośrednio z repozytorium Hugging Face Hub. Użytkownik inicjuje ten proces poprzez wywołanie funkcji load dataset, przekazując jako parametr nazwę zbioru danych. Biblioteka automatycznie pobiera wymagane pliki, wykrywa obsługiwany format danych oraz zapisuje je w lokalnym cache. Użytkownik może dodatkowo określić konfigurację zbioru danych oraz wybrany podział, na przykład zbiór treningowy lub testowy. Po zakończeniu operacji dane zwracane są w postaci struktury DataFrame, gotowej do dalszego przetwarzania.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\textwidth]{zdjęcia/ladowanie1.png}
  \caption[Przykład z livebook]{Przykład z livebook - ładowanie z Hugging Face}
  \label{fig:schemat-systemu}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\textwidth]{zdjęcia/ladowanie3.png}
  \caption[Przykład z livebook]{Przykład z livebook - ładowanie z podanego folderu}
  \label{fig:schemat-systemu}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\textwidth]{zdjęcia/ladowanie4.png}
  \caption[Przykład z livebook]{Przykład z livebook - ładowanie z tokenem autoryzacyjnym}
  \label{fig:schemat-systemu}
\end{figure}

\FloatBarrier

\subsection{Ładowanie zbiorów danych z lokalnego systemu plików}

Możliwa jest też praca z danymi zapisanymi lokalnie. W tym przypadku użytkownik wskazuje ścieżkę do katalogu zawierającego pliki danych, a biblioteka automatycznie rozpoznaje ich format oraz filtruje je zgodnie z wybraną konfiguracją i podziałem. Rozwiązanie to umożliwia jednolity sposób pracy z danymi niezależnie od ich źródła pochodzenia.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\textwidth]{zdjęcia/ladowanie2.png}
  \caption[Przykład z livebook]{Przykład z livebook}
  \label{fig:schemat-systemu}
\end{figure}

\FloatBarrier

\subsection{Wykorzystanie mechanizmu cache’owania i trybu offline}

W celu zwiększenia wydajności oraz ograniczenia liczby operacji sieciowych biblioteka automatycznie wykorzystuje mechanizm cache’owania pobranych zbiorów danych. Użytkownik może skonfigurować katalog cache za pomocą zmiennej środowiskowej oraz wybrać tryb pobierania danych, w tym ponowne użycie istniejącego cache lub wymuszenie ponownego pobrania plików. Dodatkowo możliwe jest uruchomienie trybu offline, w którym biblioteka korzysta wyłącznie z lokalnie zapisanych danych, co pozwala na pracę bez aktywnego połączenia z Internetem.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\textwidth]{zdjęcia/offline.png}
  \caption[Przykład z livebook]{Przykład z livebook - offline mode}
  \label{fig:schemat-systemu}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\textwidth]{zdjęcia/cache.png}
  \caption[Przykład z livebook]{Przykład z livebook - używanie podanego folderu jako cache}
  \label{fig:schemat-systemu}
\end{figure}

\FloatBarrier

\subsection{Strumieniowe przetwarzanie dużych zbiorów danych}

W przypadku pracy z bardzo dużymi zbiorami danych użytkownik może skorzystać z trybu strumieniowego. Po włączeniu tej opcji biblioteka zwraca strumień danych zamiast pełnego zbioru w pamięci operacyjnej. Dane są pobierane i przetwarzane progresywnie w konfigurowalnych paczkach, co umożliwia efektywną pracę z danymi przekraczającymi rozmiar dostępnej pamięci RAM. Strumień może być dalej przetwarzany przy użyciu standardowych mechanizmów języka Elixir, takich jak moduły Stream i Enum.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\textwidth]{zdjęcia/streaming.png}
  \caption[Przykład z livebook]{Przykład z livebook - wykorzystanie streamingu}
  \label{fig:schemat-systemu}
\end{figure}

\FloatBarrier

\subsection{Równoległe ładowanie i przetwarzanie danych}

Biblioteka umożliwia również wykorzystanie przetwarzania równoległego podczas ładowania zbiorów danych składających się z wielu plików. Użytkownik może określić liczbę procesów wykorzystywanych do tego celu, co pozwala na znaczące skrócenie czasu ładowania danych na systemach wielordzeniowych. Mechanizm ten jest w pełni transparentny dla użytkownika i zachowuje kolejność przetwarzanych danych.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\textwidth]{zdjęcia/threads.png}
  \caption[Przykład z livebook]{Przykład z livebook - ładowanie wielowątkowe}
  \label{fig:schemat-systemu}
\end{figure}

\FloatBarrier

\subsection{Publikowanie zbiorów danych w repozytorium Hugging Face}

Biblioteka ElixirDatasets umożliwia publikowanie zbiorów danych w repozytorium Hugging Face bezpośrednio z poziomu języka Elixir. Użytkownik przygotowuje dane w postaci struktury DataFrame, a następnie inicjuje proces ich wysyłania do wskazanego repozytorium. Biblioteka automatycznie obsługuje zapis danych w wybranym formacie oraz komunikację z interfejsem API Hugging Face, w tym proces autoryzacji i tworzenie commitów.

Dla większych zbiorów danych dostępne jest wsparcie dla przesyłania plików z wykorzystaniem technologii Git LFS, co umożliwia wydajny upload danych przekraczających limity standardowego API. Dostępna jest również możliwość zarządzania zawartością repozytorium, w tym usuwania nieaktualnych plików. Funkcjonalność ta pozwala traktować Hugging Face Hub nie tylko jako źródło danych, ale także jako platformę do ich publikowania i współdzielenia.


\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\textwidth]{zdjęcia/upload1.png}
  \caption[Przykład z livebook]{Przykład z livebook - publikowanie zbioru danych}
  \label{fig:schemat-systemu}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\textwidth]{zdjęcia/upload2.png}
  \caption[Przykład z livebook]{Przykład z livebook - publikowanie zbioru danych}
  \label{fig:schemat-systemu}
\end{figure}

\FloatBarrier

\subsection{Pobieranie informacji i metadanych o zbiorach danych}

Biblioteka zapewnia dostęp do szczegółowych informacji i metadanych dotyczących zbiorów danych przechowywanych w repozytorium Hugging Face. Użytkownik może pobrać informacje o dostępnych konfiguracjach, podziałach zbiorów, liczbie przykładów oraz podstawowe dane opisowe.

Pozyskane metadane mogą być wykorzystywane do analizy struktury zbioru danych przed jego załadowaniem lub do automatycznego doboru konfiguracji i podziałów w dalszych etapach przetwarzania. Informacje te mogą być również zapisywane lokalnie i ponownie wykorzystywane bez konieczności każdorazowej komunikacji z interfejsem API, co zwiększa przejrzystość i efektywność pracy z danymi.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\textwidth]{zdjęcia/info.png}
  \caption[Przykład z livebook]{Przykład z livebook - używanie podanego folderu jako cache}
  \label{fig:schemat-systemu}
\end{figure}

\FloatBarrier

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dalszy rozwój projektu}
\label{sec:dalszy-rozwoj-projektu}

Potencjalny dalszy rozwój biblioteki ElixirDatasets może obejmować rozszerzenia zwiększające jej funkcjonalność oraz użyteczność w zastosowaniach związanych z analizą danych i uczeniem maszynowym. Jednym z istotnych kierunków rozwoju jest wprowadzenie mechanizmów wstępnego przetwarzania danych, takich jak normalizacja i standaryzacja, obejmujących m.in. ujednolicanie danych tekstowych oraz skalowanie danych numerycznych. Naturalnym uzupełnieniem tych funkcjonalności byłaby implementacja tokenizacji danych tekstowych, stanowiącej podstawowy etap przygotowania danych w zadaniach przetwarzania języka naturalnego.

Dalsze prace mogłyby również dotyczyć rozszerzenia wsparcia dla danych multimodalnych, takich jak obrazy czy nagrania dźwiękowe, a także pogłębionej integracji z bibliotekami uczenia maszynowego dostępnymi w ekosystemie języka Elixir. W obszarze wydajności możliwe jest dalsze rozwijanie mechanizmów przetwarzania strumieniowego i równoległego oraz rozbudowa systemu cache’owania, w tym automatyczne zarządzanie przechowywanymi danymi. Realizacja tych kierunków rozwoju mogłaby znacząco zwiększyć skalowalność biblioteki oraz jej przydatność w środowiskach produkcyjnych i badawczych.

\begin{itemize}
  \item \textbf{Rozszerzenie obsługi formatów danych} – dodanie wsparcia dla kolejnych popularnych formatów, takich jak Avro, ORC czy formaty binarne wykorzystywane w zadaniach uczenia głębokiego, co zwiększyłoby uniwersalność biblioteki.

  \item \textbf{Zaawansowana walidacja i analiza jakości danych} – implementacja mechanizmów sprawdzania spójności schematów, wykrywania brakujących lub niepoprawnych wartości oraz podstawowej analizy statystycznej zbiorów danych przed ich dalszym przetwarzaniem.

  \item \textbf{Integracja z pipeline’ami uczenia maszynowego} – umożliwienie bezpośredniego łączenia procesów ładowania i przetwarzania danych z etapami trenowania modeli, np. poprzez integrację z bibliotekami opartymi na Nx i Axon.

  \item \textbf{Interfejs wiersza poleceń (CLI)} – stworzenie narzędzia CLI pozwalającego na pobieranie, publikowanie i zarządzanie zbiorami danych bez konieczności pisania kodu, co ułatwiłoby wykorzystanie biblioteki w procesach automatyzacji.

  \item \textbf{Rozbudowa mechanizmów zarządzania cache} – wprowadzenie limitów rozmiaru cache, automatycznego usuwania nieużywanych danych oraz możliwości współdzielenia cache pomiędzy różnymi projektami.

  \item \textbf{Obsługa wersjonowania zbiorów danych} – umożliwienie łatwego przełączania się pomiędzy różnymi wersjami zbiorów danych oraz śledzenia zmian, co jest szczególnie istotne w projektach badawczych i produkcyjnych.

  \item \textbf{Rozszerzenie dokumentacji i przykładów użycia} – dodanie bardziej rozbudowanych przykładów, scenariuszy zastosowań oraz integracji z popularnymi narzędziami do analizy danych.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Subiektywna ocena projektu}
\label{sec:subiektywna-ocena-projektu}

Większość kluczowych funkcjonalności, dzięki czemu powstała biblioteka umożliwiająca sprawną i wydajną obsługę zbiorów danych z repozytorium Hugging Face w języku Elixir. Zaimplementowane rozwiązania spełniają założone wymagania funkcjonalne i pozwalają na wygodną pracę zarówno z małymi, jak i dużymi zbiorami danych.

Przeprowadzone testy jednostkowe i integracyjne przyczyniły się do zapewnienia stabilności oraz wysokiej jakości implementacji. Zastosowane technologie i narzędzia programistyczne okazały się trafnym wyborem i dobrze sprawdziły się w kontekście realizowanego projektu.

Podsumowując, projekt spełnił zakładane cele i stanowi solidną podstawę do dalszego rozwoju biblioteki oraz jej wykorzystania w kolejnych projektach związanych z analizą danych i uczeniem maszynowym w języku Elixir.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Podsumowanie}
\label{sec:podsumowanie}

Niniejsza praca inżynierska przedstawia projekt i implementację biblioteki ElixirDatasets, której celem jest umożliwienie wygodnej i wydajnej pracy ze zbiorami danych pochodzącymi z repozytorium Hugging Face w języku Elixir. W ramach pracy zaprojektowano i zaimplementowano kluczowe mechanizmy ładowania danych, cache’owania, przetwarzania strumieniowego oraz równoległego, a także integrację z interfejsem API Hugging Face.

Zrealizowane rozwiązanie spełnia założone wymagania funkcjonalne i pozwala na efektywną obsługę zarówno małych, jak i dużych zbiorów danych. Przeprowadzone testy potwierdziły poprawność i stabilność działania biblioteki. Uzyskane rezultaty stanowią solidną podstawę do dalszego rozwoju projektu oraz jego wykorzystania w kolejnych pracach związanych z analizą danych i uczeniem maszynowym w ekosystemie języka Elixir.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\printbibliography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\listoffigures
\listoftables
\lstlistoflistings

\end{document}
